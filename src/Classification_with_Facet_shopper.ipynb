{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with FACET\n",
    "\n",
    "***\n",
    "\n",
    "FACET is composed of the following key components:\n",
    "\n",
    "- **Model Inspection**\n",
    "\n",
    "    FACET introduces a new algorithm to quantify dependencies and interactions between features in ML models. This new tool for human-explainable AI adds a new, global perspective to the observation-level explanations provided by the popular [SHAP](https://shap.readthedocs.io/en/latest/) approach. To learn more about FACET's model inspection capabilities, see the getting started example below.\n",
    "\n",
    "\n",
    "- **Model Simulation**\n",
    "\n",
    "    FACET's model simulation algorithms use ML models for *virtual experiments* to help identify scenarios that optimise predicted  outcomes. To quantify the uncertainty in simulations, FACET utilises a range of bootstrapping algorithms including stationary and stratified bootstraps. For an example of FACETâ€™s bootstrap simulations, see the getting started example below.    \n",
    "    \n",
    "    \n",
    "- **Enhanced Machine Learning Workflow**  \n",
    "\n",
    "    FACET offers an efficient and transparent machine learning workflow, enhancing [scikit-learn]( https://scikit-learn.org/stable/index.html)'s tried and tested pipelining paradigm with new capabilities for model selection, inspection, and simulation. FACET also introduces [sklearndf](https://github.com/BCG-Gamma/sklearndf), an augmented version of *scikit-learn* with enhanced support for *pandas* dataframes that ensures end-to-end traceability of features.    \n",
    "\n",
    "***\n",
    "\n",
    "**Context**\n",
    "\n",
    "Utilizing FACET, we will do the following:\n",
    "\n",
    "1. create a pipeline to find identify a well-performing classifier.\n",
    "2. perform model inspection and simulation to gain understanding and insight into key factors\n",
    "\n",
    "***\n",
    "\n",
    "**Tutorial outline**\n",
    "\n",
    "1. [Required imports](#Required-imports)\n",
    "2. [Preprocessing and initial feature selection](#Preprocessing-and-initial-feature-selection)\n",
    "3. [Selecting a learner using FACET ranker](#Selecting-a-learner-using-FACET-ranker)\n",
    "4. [Using FACET for advanced model inspection](#Using-FACET-for-advanced-model-inspection)\n",
    "5. [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "# ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "\n",
    "import warnings\n",
    "import tableone # need to import this to suppress an IPython warning triggered by tableone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "warnings.filterwarnings(\"ignore\", message=r\".*`should_run_async` will not call `transform_cell`\")\n",
    "warnings.filterwarnings(\"ignore\", message=r\".*`np\\..*` is a deprecated alias\")\n",
    "\n",
    "\n",
    "# set global options for matplotlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, we will import not only the FACET package, but also other packages useful to solve this task. Overall, we can break down the imports into three categories: \n",
    "\n",
    "1. Common packages (pandas, matplotlib, etc.)\n",
    "2. Required FACET classes (inspection, selection, validation, simulation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common package imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import tableone\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FACET imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facet.data import Sample\n",
    "from facet.inspection import LearnerInspector\n",
    "from facet.selection import LearnerRanker, LearnerGrid\n",
    "from facet.validation import BootstrapCV\n",
    "from facet.data.partition import ContinuousRangePartitioner\n",
    "from facet.simulation import UnivariateProbabilitySimulator\n",
    "from facet.simulation.viz import SimulationDrawer\n",
    "from facet.crossfit import LearnerCrossfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearndf imports**\n",
    "\n",
    "Instead of using the \"regular\" scikit-learn package, we are going to use sklearndf (see on [GitHub](https://github.com/orgs/BCG-Gamma/sklearndf/)). sklearndf is an open source library designed to address a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names. sklearndf retains all the functionality available through scikit-learn plus the feature traceability and usability associated with Pandas data frames. Additionally, the names of all your favourite scikit-learn functions are the same except for `DF` on the end. For example, the standard scikit-learn import:\n",
    "\n",
    "`from sklearn.pipeline import Pipeline`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`from sklearndf.pipeline import PipelineDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearndf.pipeline import PipelineDF, ClassifierPipelineDF\n",
    "from sklearndf.classification import RandomForestClassifierDF\n",
    "from sklearndf.classification.extra import LGBMClassifierDF\n",
    "from sklearndf.transformation import (\n",
    "    ColumnTransformerDF,\n",
    "    OneHotEncoderDF,\n",
    "    SimpleImputerDF,\n",
    ")\n",
    "from sklearndf.transformation.extra import BorutaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytools imports**\n",
    "\n",
    "pytools (see on [GitHub](https://github.com/BCG-Gamma/pytools)) is an open source library containing general machine learning and visualization utilities, some of which are useful for visualising the advanced model inspection capabilities of FACET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytools.viz.dendrogram import DendrogramDrawer, LinkageTree\n",
    "from pytools.viz.matrix import MatrixDrawer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and initial feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load our data and create a simple preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prepared data frame\n",
    "shop_df = pd.read_csv(\"online_shopers_intention.csv\")\n",
    "# have a look\n",
    "shop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure a quick run we will use a random sample of 1000 observations\n",
    "shop_df = shop_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "For easier data management we will create a sample object using FACET's `Sample` class, which allows us to: \n",
    "\n",
    "- Quickly access the target vs. features\n",
    "- Pass our data into sklearndf pipelines\n",
    "- Pass information to other FACET functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a FACET sample object\n",
    "shop = Sample(\n",
    "    observations=shop_df,\n",
    "    feature_names=shop_df.drop(columns=[\"Revenue\"]).columns,\n",
    "    target_name=\"Revenue\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "Next we create a minimum preprocessing pipeline which based on our initial EDA\n",
    "\n",
    "1. Simple imputation for missing values in both continuous and categorical features\n",
    "2. One-hot encoding for categorical features\n",
    "\n",
    "We will use the sklearndf wrappers for scikit-learn functions such as `SimpleImputerDF` in place of `SimpleImputer`, `OneHotEncoderDF` in place of `OneHotEncoder`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical features we will use the mode as the imputation value and also one-hot encode\n",
    "preprocessing_categorical = PipelineDF(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputerDF(strategy=\"most_frequent\", fill_value=\"<na>\")),\n",
    "        (\"one-hot\", OneHotEncoderDF(sparse=False, handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for numeric features we will impute using the median\n",
    "preprocessing_numerical = SimpleImputerDF(strategy=\"median\")\n",
    "\n",
    "# put the pipeline together\n",
    "preprocessing_features = ColumnTransformerDF(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"categorical\",\n",
    "            preprocessing_categorical,\n",
    "            make_column_selector(dtype_include=object),\n",
    "        ),\n",
    "        (\n",
    "            \"numerical\",\n",
    "            preprocessing_numerical,\n",
    "            make_column_selector(dtype_include=np.number),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform some initial feature selection using Boruta, a recent approach shown to have quite good performance. The Boruta algorithm removes features that are no more predictive than random noise. If you are interested further, please see this  [article](https://www.jstatsoft.org/article/view/v036i11).\n",
    "\n",
    "The `BorutaDF` transformer in our sklearndf package provides easy access to this powerful method. The approach relies on a tree-based learner, usually a random forest. For settings, a `max_depth` of between 3 and 7 is typically recommended, and here we rely on the default setting of 5. However, as this depends on the number of features and the complexity of interactions one could also explore the sensitivity of feature selection to this parameter. The number of trees is automatically managed by the Boruta feature selector argument `n_estimators=\"auto\"`.\n",
    "\n",
    "We also use parallelization for the random forest using `n_jobs` to accelerate the Boruta iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the pipeline for Boruta\n",
    "boruta_feature_selection = PipelineDF(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing_features),\n",
    "        (\n",
    "            \"boruta\",\n",
    "            BorutaDF(\n",
    "                estimator=RandomForestClassifierDF(\n",
    "                    max_depth=5, n_jobs=-3, random_state=42\n",
    "                ),\n",
    "                n_estimators=\"auto\",\n",
    "                random_state=42,\n",
    "                verbose=False,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run feature selection using Boruta and report those selected\n",
    "boruta_feature_selection.fit(X=ahop.features, y=shop.target)\n",
    "selected = boruta_feature_selection.feature_names_original_.unique()\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta identified 10 features (out of a potential 47) that we will retain in our FACET sample object for classification. Note that this feature selection process could be included in a general preprocessing pipeline, however due to the computation involved, we have utilized Boruta here as an initial one-off processing step to narrow down the features for our classifier development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update FACET sample object to only those features Boruta identified as useful\n",
    "shop_initial_features = shop.keep(feature_names=selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a learner using FACET ranker\n",
    "\n",
    "FACET implements several additional useful wrappers which further simplify comparing and tuning a larger number of models and configurations: \n",
    "\n",
    "- `LearnerGrid`: allows you to pass a learner pipeline (i.e., classifier + any preprocessing) and a set of hyperparameters\n",
    "- `LearnerRanker`: multiple LearnerGrids can be passed into this class as a list - this allows tuning hyperparameters both across different types of learners in a single step and ranks the resulting models accordingly\n",
    "\n",
    "The following learners and hyperparameter ranges will be assessed using 10 repeated 5-fold cross-validation:\n",
    "\n",
    "\n",
    "1. **Random forest**: with hyperparameters\n",
    "    - max_depth: [4, 5, 6]\n",
    "    - min_samples_leaf: [8, 11, 15]  \n",
    "  \n",
    "  \n",
    "2. **Light gradient boosting**: with hyperparameters\n",
    "    - max_depth: [4, 5, 6]\n",
    "    - min_samples_leaf: [8, 11, 15]  \n",
    "\n",
    "Note if you want to see a list of hyperparameters you can use `classifier_name().get_params().keys()` where `classifier_name` could be for example `RandomForestClassifierDF` and if you want to see the default values, just use `classifier_name().get_params()`.\n",
    "\n",
    "Finally, for this exercise we will use AUC as the performance metric for scoring and ranking our classifiers (the default is accuracy). Note that ranking uses the average performance minus two times the standard deviation, so that we consider both the average performance and variability when selecting a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the classifiers we want to train using `ClassifierPipelineDF` from sklearndf. Note here we also include the feature preprocessing steps we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest learner\n",
    "rforest_clf = ClassifierPipelineDF(\n",
    "    preprocessing=preprocessing_features,\n",
    "    classifier=RandomForestClassifierDF(random_state=42),\n",
    ")\n",
    "\n",
    "# light gradient boosting learner\n",
    "lgbm_clf = ClassifierPipelineDF(\n",
    "    preprocessing=preprocessing_features,\n",
    "    classifier=LGBMClassifierDF(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a list of learner grids where each learner grid is created using `LearnerGrid` and allows us to associate a `ClassifierPipelineDF` with a specified set of hyperparameter via the `learner_parameters` argument. Note this structure allows us to easily include additional classifiers and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rforest_clf,\n",
    "        learner_parameters={\n",
    "            \"max_depth\": [4, 5, 6], \n",
    "            \"min_samples_leaf\": [8, 11, 15],\n",
    "        },\n",
    "    ),\n",
    "    LearnerGrid(\n",
    "        pipeline=lgbm_clf,\n",
    "        learner_parameters={\n",
    "            \"max_depth\": [4, 5, 6],\n",
    "            \"min_samples_leaf\": [8, 11, 15],\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the grid defined above using the `LeanerRanker`, which will run a gridsearch (or random search if defined) using 10 repeated 5-fold cross-validation on our selected set of features from Boruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_ranker = LearnerRanker(\n",
    "    grids=classifier_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    scoring=\"roc_auc\",\n",
    ").fit(shop_initial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how each model scored using the `summary_report()` method of the `LearnerRanker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at performance for the top ranked classifiers\n",
    "clf_ranker.summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see based on our learner ranker, we have selected a Random Forest algorithm that achieved a mean ROC AUC of 0.72 with a SD of 0.03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FACET for advanced model inspection\n",
    "\n",
    "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for each observation (i.e., explain **individual** predictions).\n",
    "\n",
    "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding pairwise feature redundancy and synergy. Redundancy and synergy are calculated using a new algorithm to understand model predictions from a **global perspective** to complement local SHAP.\n",
    "\n",
    "The definitions of synergy and redundancy are as follows:\n",
    "\n",
    "- **Synergy**\n",
    "\n",
    "  The degree to which the model combines information from one feature with \n",
    "  another to predict the target. For example, let's assume we are predicting \n",
    "  cardiovascular health using age and gender and the fitted model includes \n",
    "  a complex interaction between them. This means these two features are \n",
    "  synergistic for predicting cardiovascular health. Further, both features \n",
    "  are important to the model and removing either one would significantly \n",
    "  impact performance. Let's assume age brings more information to the joint\n",
    "  contribution than gender. This asymmetric contribution means the synergy for\n",
    "  (age, gender) is less than the synergy for (gender, age). To think about it\n",
    "  another way, imagine the prediction is a coordinate you are trying to reach.\n",
    "  From your starting point, age gets you much closer to this point than \n",
    "  gender, however, you need both to get there. Synergy reflects the fact \n",
    "  that gender gets more help from age (higher synergy from the perspective \n",
    "  of gender) than age does from gender (lower synergy from the perspective of\n",
    "  age) to reach the prediction. *This leads to an important point: synergy \n",
    "  is a naturally asymmetric property of the global information two interacting \n",
    "  features contribute to the model predictions.* Synergy is expressed as a \n",
    "  percentage ranging from 0% (full autonomy) to 100% (full synergy).\n",
    "\n",
    "\n",
    "- **Redundancy**\n",
    "\n",
    "  The degree to which a feature in a model duplicates the information of a \n",
    "  second feature to predict the target. For example, let's assume we had \n",
    "  house size and number of bedrooms for predicting house price. These \n",
    "  features capture similar information as the more bedrooms the larger \n",
    "  the house and likely a higher price on average. The redundancy for \n",
    "  (number of bedrooms, house size) will be greater than the redundancy \n",
    "  for (house size, number of bedrooms). This is because house size \n",
    "  \"knows\" more of what number of bedrooms does for predicting house price \n",
    "  than vice-versa. Hence, there is greater redundancy from the perspective \n",
    "  of number of bedrooms. Another way to think about it is removing house \n",
    "  size will be more detrimental to model performance than removing number \n",
    "  of bedrooms, as house size can better compensate for the absence of \n",
    "  number of bedrooms. This also implies that house size would be a more \n",
    "  important feature than number of bedrooms in the model. *The important \n",
    "  point here is that like synergy, redundancy is a naturally asymmetric \n",
    "  property of the global information feature pairs have for predicting \n",
    "  an outcome.* Redundancy is expressed as a percentage ranging from 0% \n",
    "  (full uniqueness) to 100% (full redundancy).\n",
    "\n",
    "\n",
    "Note that cases can apply at the same time so a feature pair can use some information synergistically and some information redundantly.\n",
    "\n",
    "The FACET `LearnerInspector` can calculate all of this with a single method call, but also offers methods to access the intermediate results of each step. A lightweight visualization framework is available to render the results in different styles.\n",
    "\n",
    "SHAP values from the `LearnerInspector` can also be used with the SHAP package plotting functions for sample and observation level SHAP visualizations, such as SHAP distribution plots, dependency plots, force plots and waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run inspector\n",
    "clf_inspector = LearnerInspector(\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(crossfit=clf_ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Feature importance has many ways of being measured. Here we utilize the FACET implementation based on the `LearnerInspector`. Each feature is ranked according to the mean SHAP value for that feature. This plot is paired with a standard SHAP distribution plot for features to see if there is any directional tendency for the associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACET feature importance\n",
    "f_importance = clf_inspector.feature_importance()\n",
    "plt.subplot(1, 2, 1)\n",
    "f_importance.sort_values().plot.barh()\n",
    "\n",
    "# get some info for standard SHAP model inspection\n",
    "shap_data = clf_inspector.shap_plot_data()\n",
    "\n",
    "# standard SHAP summary plot using the shap package\n",
    "plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(shap_values=shap_data.shap_values, features=shap_data.features, show=False, plot_size=(16.0, 8.0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance's we can see the top five features are age, RBC count, waist to height ratio, average systolic blood pressure and waist circumference. Inspection of the SHAP value distributions does not provide any indication of a general direction of association for any features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synergy heatmap\n",
    "synergy_matrix = clf_inspector.feature_synergy_matrix()\n",
    "MatrixDrawer(style=\"matplot%\").draw(synergy_matrix, title=\"Feature synergies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy heatmap\n",
    "redundancy_matrix = clf_inspector.feature_redundancy_matrix()\n",
    "MatrixDrawer(style=\"matplot%\").draw(redundancy_matrix, title=\"Feature redundancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature clustering\n",
    "\n",
    "As detailed above redundancy and synergy for a feature pair is from the \"perspective\" of one of the features in the pair, and so yields two distinct values. However, a symmetric version can also be computed that provides not only a simplified perspective but allows the use of (1 - metric) as a feature distance. With this distance hierarchical, single linkage clustering is applied to create a dendrogram visualization. This helps to identify groups of low distance, features which activate \"in tandem\" to predict the outcome. Such information can then be used to either reduce clusters of highly redundant features to a subset or highlight clusters of highly synergistic features that should always be considered together.\n",
    "\n",
    "For this example, let's apply clustering to redundancy to see how the apparent grouping observed in the heatmap appears in the dendrogram. Ideally, we want to see features only start to cluster as close to the right-hand side of the dendrogram as possible. This implies all features in the model are contributing uniquely to our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy dendrogram\n",
    "dd_redundancy = clf_inspector.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"Redundancy linkage\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience when working in a non-notebook environment, all of the `Drawer`'s provided by the [pytools](https://github.com/BCG-Gamma/pytools) package also support a `style='text'` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DendrogramDrawer(style=\"text\").draw(title=\"Redundancy linkage\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redundant features\n",
    "\n",
    "Recall the redundancy dendrogram above where we saw a clear cluster of features with redundancy; \n",
    "\n",
    "- assess if the features of the model are unique, i.e. not redundant with other features\n",
    "- decide which features to discard, combine, or modify to increase the uniqueness of important features in the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant features from our FACET sample object\n",
    "shop_no_redundant_feat = shop_initial_features.drop(\n",
    "    feature_names=[\"ExitRates\", \"Month\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run ranker without redundant features\n",
    "clf_ranker = LearnerRanker(\n",
    "    grids=classifier_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    scoring=\"roc_auc\",\n",
    ").fit(shop_no_redundant_feat)\n",
    "\n",
    "clf_ranker.summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run inspector\n",
    "inspector_no_redun = LearnerInspector(\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(crossfit=clf_ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy dendrogram\n",
    "dd_redundancy = inspector_no_redun.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"HD set features\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the removal of `BMI` and `Waist Circumference` we can see the feature clustering starts much further to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the best ranked model after removing redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ranker.best_model_.classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
